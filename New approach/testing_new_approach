import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from models import SLP, RBFN, MLP, SimpleRNN, SingleNeuron 

import random

# Function to train a model
def train_model(model, train_loader, criterion, optimizer, epochs=10):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for inputs, targets in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_loader)}")

# Function to evaluate a model
def evaluate_model(model, test_loader, criterion):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, targets in test_loader:
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            total_loss += loss.item()
            
            # Accuracy calculation for classification tasks
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == targets).sum().item()
            total += targets.size(0)
    
    avg_loss = total_loss / len(test_loader)
    accuracy = correct / total
    return avg_loss, accuracy

def sample_architecture(input_size, output_size):
    hidden_layers = random.choices([16, 32, 64, 128], k=random.randint(2, 5))
    activation_fn = random.choice([nn.ReLU, nn.LeakyReLU, nn.Sigmoid])
    dropout_rate = random.choice([0, 0.2, 0.5])
    optimizer_type = random.choice([optim.Adam, optim.SGD])
    learning_rate = random.uniform(0.001, 0.01)

    return {
        "hidden_layers": hidden_layers,
        "activation_fn": activation_fn,
        "dropout_rate": dropout_rate,
        "optimizer_type": optimizer_type,
        "learning_rate": learning_rate
    }


